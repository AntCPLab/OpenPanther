{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from time import time\n",
    "import faiss\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([206094, 113489,  56939,  31248,  17655,   9625,   5733,   3154,   1674,\n",
      "         43180])\n"
     ]
    }
   ],
   "source": [
    "dataset = \"../dataset/deep10M\"\n",
    "max_points_per_cluster = 40\n",
    "model_dict = torch.load(dataset+\".pth\")\n",
    "ids = model_dict['ids']\n",
    "cluster_ids = model_dict['cluster_idx']\n",
    "index = model_dict['index']\n",
    "centroids = model_dict['centroids']\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset load done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, \"../dataset/deep-image-96-euclidean.hdf5\")\n",
    "deep10M = h5py.File(file_path, \"r\")\n",
    "test_x = deep10M['test'][:]\n",
    "train_x = deep10M['train'][:]\n",
    "train_x = ((train_x + 1.0) * 127.5 + 0.5).astype(int)\n",
    "test_x = ((test_x + 1.0) * 127.5 + 0.5).astype(int)\n",
    "test_x = torch.from_numpy(test_x)\n",
    "train_x = torch.from_numpy(train_x)\n",
    "print(\"Dataset load done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./dataset/\"+dataset+\"_test.txt\",test_x,fmt=\"%d\",delimiter=\" \")\n",
    "np.savetxt(\"./dataset/\"+dataset+\"_dataset.txt\",train_x,fmt=\"%d\",delimiter=\" \")\n",
    "np.savetxt(\"./dataset/\"+dataset+\"_centroids.txt\",centroids, fmt = \"%d\", delimiter= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:  96\n"
     ]
    }
   ],
   "source": [
    "d=test_x.shape[1]\n",
    "print(\"dim: \",d)\n",
    "searchs = faiss.IndexFlatL2(d)\n",
    "searchs.add(train_x)\n",
    "D,res = searchs.search(test_x, 10)\n",
    "neighbors = torch.from_numpy(res)\n",
    "np.savetxt(\"./dataset/\"+dataset+\"_neighbors.txt\",neighbors,fmt = \"%d\", delimiter= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maps cluster IDs to point IDs \"\"\"\n",
    "def save_ptoc(cluster_number, max_points_per_cluster, save):\n",
    "    result = torch.empty((cluster_number, max_points_per_cluster))\n",
    "    result.fill_(111111111)\n",
    "    p_ids = ids.reshape(-1).type(torch.int)\n",
    "    c_ids = cluster_ids.reshape(-1).type(torch.int)\n",
    "    order = torch.argsort(c_ids)\n",
    "    p_ids = p_ids[order]\n",
    "    c_ids = c_ids[order] \n",
    "    counts = torch.bincount(c_ids)\n",
    "    split_p_ids = torch.split(p_ids,counts.tolist(),dim=0)\n",
    "    for i in range(cluster_number):\n",
    "        cluster_points = split_p_ids[i]\n",
    "        result[i, :len(cluster_points)] = cluster_points\n",
    "    if save == True:\n",
    "        np.savetxt(\"./dataset/\"+dataset+\"_ptoc.txt\",result,fmt=\"%d\",delimiter=\" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 488791\n",
      "Num cluster: tensor(445611)  Stash size: tensor(43180)\n"
     ]
    }
   ],
   "source": [
    "all_cluster = centroids.shape[0]\n",
    "num_cluters = sum(index) - index[-1]\n",
    "stash_size = index[-1]\n",
    "print(\"Total:\",all_cluster)\n",
    "print(\"Num cluster:\", num_cluters,\" Stash size:\", stash_size)\n",
    "ptoc = save_ptoc(num_cluters,max_points_per_cluster,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Stash\n",
    "_, stash_id = searchs.search(centroids[num_cluters:],1)\n",
    "stash_id = torch.from_numpy(stash_id)\n",
    "np.savetxt(\"./dataset/\"+dataset+\"_stash.txt\", stash_id, fmt=\"%d\", delimiter= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids: torch.Size([488791, 96])\n",
      "stash: torch.Size([43180, 1])\n",
      "dataset: torch.Size([9990000, 96])\n",
      "test: torch.Size([10000, 96])\n",
      "neighbors: torch.Size([10000, 10])\n",
      "ptoc: torch.Size([445611, 40])\n"
     ]
    }
   ],
   "source": [
    "print(\"centroids:\",centroids.shape)\n",
    "print(\"stash:\",stash_id.shape)\n",
    "print(\"dataset:\",train_x.shape)\n",
    "print(\"test:\",test_x.shape)\n",
    "print(\"neighbors:\",neighbors.shape)\n",
    "print(\"ptoc:\",ptoc.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
